#oracle


`rac-01 和 rac-02`两个节点，其中rac-02因故障，服务器无法启动 现在需要新增`rac-03`节点替换`rac-02`

### **前提条件**

1. **新节点准备** ：
	- 确保rac-03的OS版本、内核参数、用户/组（oracle、grid）、目录结构（ORACLE_HOME、GRID_HOME）与集群其他节点一致。
	- 配置网络（Public IP、Private IP、VIP、SCAN）并更新所有节点的`/etc/hosts`。
	- 配置共享存储（ASM磁盘）权限，确保rac-03可访问。
	- 建立节点间SSH互信（grid/oracle用户）。
2. **备份** ：备份集群配置文件（OCR、VF）、数据库。

[[2. Install Oracle RAC for 19c#节点免密互信]]
[[2. Install Oracle RAC for 19c#基础环境准备]]
[[2. Install Oracle RAC for 19c#添加用户环境变量]]
[[2. Install Oracle RAC for 19c#配置共享存储]]  

```bash
# 安装cvuqdisk-1.0.10-1.rpm 切换到root
dir $GRID_HOME/cv/rpm/cvuqdisk-1.0.10-1.rpm 
rpm -ivh /data/u01/app/19c/grid/cv/rpm/cvuqdisk-1.0.10-1.rpm

# 检车iscsi共享的磁盘uuid，确定与rac-01 一致
for disk in /dev/sd*; do 
  echo -n "$disk: "; 
  /lib/udev/scsi_id -g -u $disk
done
```


---


### **删除故障节点 rac-02**

#### 1. **强制删除节点（从存活节点执行，如 rac-01）**

```bash
# 以 root 用户执行
$GRID_HOME/bin/crsctl delete node -n rac-02  # 强制删除节点
# 检查是否删除
olsnodes -s
```

- 若命令失败，手动清理：
```bash
$GRID_HOME/bin/ocrconfig -delete ra-c02      # 删除OCR中的节点
$GRID_HOME/bin/olsnodes -d rac-02           # 删除节点列表
```

#### 2. **更新节点列表（Grid 和 RDBMS）**

```bash
# 在 rac-01 上以 grid 用户执行
$GRID_HOME/oui/bin/runInstaller -updateNodeList ORACLE_HOME=$GRID_HOME CLUSTER_NODES=rac-01,rac-03  # 更新Grid节点列表

# 在 rac-01 上以 oracle 用户执行
$ORACLE_HOME/oui/bin/runInstaller -updateNodeList ORACLE_HOME=$ORACLE_HOME CLUSTER_NODES=rac-01,rac-03  # 更新RDBMS节点列表
```


#### 3. 清理故障节点资源

以 root 用户执行

```bash
# 删除 VIP 资源
srvctl remove vip -i rac-02 -f

# 删除监听器（如果存在）
srvctl remove listener -listener rac-02

# 删除节点上的数据库实例
srvctl remove instance -db <db_name> -i rac-02 -f

# 如果rac-02故障后，vip飘逸到了rac-01，可以使用ifconfig ens:192:4 down 暴力删除
```


---

### **二、添加新节点 rac-03**

#### 1. **安装 Grid Infrastructure**

```bash
# 在 rac-01 上以 grid 用户执行
/data/u01/app/19c/grid/addnode/addnode.sh -silent "CLUSTER_NEW_NODES={rac-03}" "CLUSTER_NEW_VIRTUAL_HOSTNAMES={rac-03-vip}"
```

- 按提示在 rac-03 上以 **root** 执行`$GRID_HOME/root.sh`。

#### 2. **安装 Oracle RDBMS 软件**

```bash
# 在 rac-01 上以 oracle 用户执行
$ORACLE_HOME/addnode/addNode.sh -silent "CLUSTER_NEW_NODES={rac03}"
```

- 按提示在 rac-03 上以 **root** 执行`$ORACLE_HOME/root.sh`。

#### 3. **添加数据库实例**

```bash
# 在任一节点执行
srvctl add instance -db <db_name> -node rac03 -instance <instance_name>  # e.g., ORCL3
srvctl start instance -db <db_name> -instance <instance_name>
```

#### 4. **验证集群状态**

```bash
# 检查集群节点
$GRID_HOME/bin/olsnodes -n -i

# 检查资源状态
$GRID_HOME/bin/crsctl status res -t

# 检查数据库实例
srvctl status database -db <db_name>
```

---

### **关键注意事项**


1. **VIP 和 SCAN** ：确保新节点的VIP配置正确，且DNS/`/etc/hosts`已更新。
2. **ASM 磁盘组** ：验证 rac-03 对ASM磁盘的访问权限（例如：`kfod disk=all`）。
3. **日志排查** ：
	- Grid 安装日志：`$GRID_HOME/log/rac03/addNode*`
	- RDBMS 安装日志：`$ORACLE_HOME/log/rac03/addNode*`
4. **回退计划** ：若添加失败，使用`addNode.sh`的`-skipPrereqs`选项跳过检查，或回退到备份。

---

### **补充：直接添加节点（若原集群健康）**

若原集群（rac-01）运行正常，可跳过删除步骤，直接添加 rac-03：
```bash
# 在 rac-01 上执行
$GRID_HOME/addnode/addNode.sh -silent "CLUSTER_NEW_NODES={rac03}" ...
$ORACLE_HOME/addnode/addNode.sh -silent "CLUSTER_NEW_NODES={rac03}" ...
```





---



通过以上步骤，rac-03 将替代 rac-02 成为集群新节点。严格遵循Oracle文档（[Add/Remove Nodes in RAC](https://docs.oracle.com/en/database/oracle/oracle-database/19/cwadd/)）并测试环境后再操作生产集群。