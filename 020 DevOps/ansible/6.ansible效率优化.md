# 6.ansible效率优化

Ansible虽然方便，但有个”为人诟病”的问题：任务执行速度太慢了，在有大量任务、大量循环任务时，其速度之慢真的是会让人等到崩溃的。

Ansible官方给了一些优化选项供用户选择，还可以去网上寻找优化Ansible相关的插件。但在调优Ansible之前，应当先去**理解Ansible的执行流程**，如此才能知道为什么速度慢、要如何调优以及调优后会造成什么后果。此外，还应学会**测量任务的执行速度**。

此外，本文还会回顾部分Ansible执行策略，更详细的执行策略说明，可复习第十章的”理解Ansible执行策略”部分。

## 1. 测量任务执行速度：profile\_tasks插件

Ansible官方提供了几个可用于计时的回调插件：

- (1).`profile_tasks`​：该回调插件用于计时每个任务的执行时长
- (2).`profile_roles`​插件用于计时每个Role的执行时长
- (3).`timer`​插件用于计时每个play执行时长

要使用这些插件，需要在ansible.cfg配置文件中的`callback_whitelist`​中加入各插件。如下：

```bash
[defaults]
callback_whitelist = profile_tasks
# callback_whitelist = profile_tasks, profile_roles, timer
```

上面我只开启了`profile_tasks`​插件。

这些回调插件会将对应的计时信息输出，通过观察这些计时信息，便可以知道任务执行消耗了多长时间，并多次比对计时信息，从而可确定哪种方式更高效。

然后执行几个任务看看输出结果，如下playbook文件内容：

```bash
---
- name: test for timer
  hosts: timer
  gather_facts: no
  tasks:
    - name: only one debug
      debug: 
        var: inventory_hostname
    
    - name: shell
      shell:
        cp /etc/fstab /tmp/
      loop: "{{ range(0, 100)|list }}"

    - name: scp
      copy:
        src: /etc/hosts
        dest: /tmp/
      loop: "{{ range(0, 100)|list }}"

```

其中timer主机组有三个节点，所以整个playbook中，每个节点执行201次任务，总共执行603次任务。以下是开启`profile_tasks`​后在屏幕中输出的计时信息：

```bash
$ ansible-playbook -i timer.host timer.yml
...................
......省略输出......
...................
=========================================
scp ------------------------------------ 57.96s
shell ---------------------------------- 42.78s
only one debug ------------------------- 0.07s

```

从结果中可看到，3个节点的debug任务总共花费0.07秒，3个节点的shell任务总共300次任务花费42.78秒，3个节点的scp任务总共300次任务花费57.96秒。

## 2. Ansible执行流程分析

ansible命令或ansible-playbook命令加上`-vvv`​选项，会输出很多调试信息，包括建立的连接、发送的文件等等。

例如，下面是Ansible 2.9默认配置中执行**每单个任务**都涉及到的步骤，其中我省略了大量信息以便各位能够看懂关键步骤。各位可自行加上`-vvv`​去执行一个任务并观察输出信息，同时可与我所做的注释做比较。

> 需注意：不同版本的Ansible为每个任务建立的连接数量不同，Ansible 2.9为每个任务建立7次ssh连接。有的资料或书籍中介绍时说只建立二次、三次、四次ssh连接都是有可能的，版本不同确实是有区别的。

```bash
# 1.第一个连接：获取用户家目录，此处为/root
<node1> ESTABLISH SSH CONNECTION FOR USER: None
<node1> SSH: EXEC ssh -vvv ....... '/bin/sh -c '"'"'echo ~ && sleep 0'"'"''
<node1> (0, '/root\n', ......)

# 2.第二个连接：在家目录下创建临时目录，临时目录由配置文件中remote_tmp指令控制
<node1> ESTABLISH SSH CONNECTION FOR USER: None
<node1> SSH: EXEC ssh -vvv ...... '/bin/sh -c '"'"'( umask 77 && mkdir -p "` echo /root/.ansible/tmp/ansible-tmp-1575542743.85-116022411851390 `" && ...... `" ) && sleep 0'"'"''

# 3.第三个连接：探测目标节点的平台和python解释器的版本信息
<node1> Attempting python interpreter discovery
<node1> ESTABLISH SSH CONNECTION FOR USER: None
<node1> SSH: EXEC ssh -vvv ......

# 4.第四个连接：将要执行的模块相关的代码和参数放到本地临时文件中，并使用sftp将任务文件传输到被控节点的临时文件中
<node1> ESTABLISH SSH CONNECTION FOR USER: None
<node1> SSH: EXEC ssh -vvv ......
Using module file /usr/lib/python2.7/site-packages/ansible/modules/system/ping.py
......
<node1> SSH: EXEC sftp ......
<node1> (0, 'sftp> put /root/.ansible/tmp/ansible-local-78628na2FKL/tmpaE1RbJ /root/.ansible/tmp/ansible-tmp-1575542743.85-116022411851390/AnsiballZ_ping.py\n', ......

# 5.第五个连接：对目标节点上的任务文件授以执行权限
<node1> ESTABLISH SSH CONNECTION FOR USER: None
<node1> SSH: EXEC ssh -vvv ...... '/bin/sh -c '"'"'chmod u+x /root/.ansible/tmp/ansible-tmp-1575542743.85-116022411851390/ /root/.ansible/tmp/ansible-tmp-1575542743.85-116022411851390/AnsiballZ_ping.py && sleep 0'"'"''
......

# 6.第六个连接：执行目标节点上的任务
<node1> ESTABLISH SSH CONNECTION FOR USER: None
<node1> SSH: EXEC ssh -vvv ...... '/bin/sh -c '"'"'/usr/bin/python /root/.ansible/tmp/ansible-tmp-1575542743.85-116022411851390/AnsiballZ_ping.py && sleep 0'"'"''
<node1> (0, '\r\n{"invocation": {"module_args": {"data": "pong"}}, "ping": "pong"}\r\n', 
......

# 7.第七个连接：删除目标节点上的临时目录
<node1> ESTABLISH SSH CONNECTION FOR USER: None
<node1> SSH: EXEC ssh -vvv ...... '/bin/sh -c '"'"'rm -f -r /root/.ansible/tmp/ansible-tmp-1575542743.85-116022411851390/ > /dev/null 2>&1 && sleep 0'"'"''
......

```

总结一下Ansible为**每单个任务**建立7次ssh连接所作的事情：

- (1).第一个连接：获取远程主机时行目标用户的家目录，此处为/root
- (2).第二个连接：在远程家目录下创建临时目录，临时目录可由ansible.cfg中`remote_tmp`​指令控制
- (3).第三个连接：探测目标节点的平台和python解释器的版本信息
- (4).第四个连接：将待执行模块的相关代码和参数放到本地临时文件中，并使用sftp将任务文件传输到被控节点的临时文件中
- (5).第五个连接：对目标节点上的任务文件授以执行权限
- (6).第六个连接：执行目标节点上的任务
- (7).第七个连接：删除目标节点上的临时目录，并将执行结果返回给Ansible端

从单个任务的执行流程跳出来，更全局一点，那么整个执行流程(默认配置下)大致如下(不考虑inventory阶段或执行完任务后的回调阶段，只考虑执行的任务流程)：

- (1).进入第一个play，挑选forks=5设置的5个节点
- (2).每个节点执行第一个任务，每个节点都会建立7次ssh连接
- (3).每个节点执行第二个任务，每个节点都再次建立7次ssh连接
- (4).按照相同逻辑执行该play中其它任务…
- (5).所有节点执行完该play中的所有任务后，进入下一个play
- (6).按照上面的流程执行完所有play中的所有任务

以上便是整个执行流程，各位大概也看出来了，Ansible在建立ssh连接方面上实在是”不遗余力”，可能是因为Ansible官方团队太爱ssh了……开玩笑的啦……。

## 3. 回顾Ansible的执行策略

使用forks、serial、strategy等指令可以改变Ansible的执行策略。

默认情况下forks=5，这表明在某一时刻最多只有5个执行任务的工作进程(还有一个主进程)，也即最多只能挑选5个节点同时执行任务。

​`serail`​是play级别的指令，用于指定几个节点作为一批去执行该play，该play执行完后才让下一批节点执行该play中的任务。如果不指定serial，则默认的行为等价于将所有节点当作一批。

​`strategy`​指令用于指定节点执行任务时的策略，其侧重点在于节点而在于任务，默认情况下其策略为`linear`​，表示某个节点先执行完一个任务后等待其余所有节点都执行完该任务，才统一进入下一个任务。另一种策略是`free`​策略，表示某节点执行完一个任务后不等待其它节点，而是毫不停留的继续执行该play中的剩余任务，直到该play执行完成，才释放节点槽位让其它未执行任务的节点开始执行任务。

---

## 4. 加大forks的值

默认forks=5，即最多5个Ansible工作进程。即便是只有6个目标节点，对于"效率"常挂嘴边的21世纪IT人来说，这么一点数量的工作进程，也显然是杯水车薪。

对于Ansible来说，它的的大多数任务都是分派给目标节点去执行的，所以控制端通常比较闲暇。换句话说，控制端在浪费宝贵的资源。因此，如果没有其它资源的瓶颈(比如网络带宽瓶颈、磁盘IO瓶颈)，Ansible控制端可以尽可能开大马力，让足够多的节点同时开始运行。

加大forks的值，甚至尽情地加大forks的值，可以很大幅度地提升整个任务的执行效率。不用担心太多Ansible工作进程数量会影响Ansible控制端的性能，Ansible工作进程那缓慢的工作量对OS来说实在太轻松了(除非是一些会占用大量资源的特殊任务)。我想，对于一般任务来说，唯一需要考虑的是网络带宽是否足够支撑足够数量目标节点，偶尔可能还需要考虑磁盘瓶颈。

## 5. 修改执行策略

默认情况下Ansible会让所有节点(或者serial指定的数量)执行完同一个任务后才让它们进入下一个任务，这体现了各节点的公平性和实时性：每个节点都能尽早执行到任务。这其实和操作系统的进程调度是类似的概念，只不过相对于操作系统的调度系统来说，Ansible的调度策略实在是太简陋了。

假设forks设置的比较大，可以一次性让足够多的节点并发执行任务，那么同时设置任务的执行策略为strategy=free便能让这些执行任务的节点彻底放飞自我。只是剩余的一部分节点可能会比较悲剧，它们处于调度不公平的一方。但是从整体来说，先让大部分节点快速完成任务是值得的。

但是要注意，有些场景下要小心使用free策略，特别是节点依赖时。比如，某些节点运行服务A，另一些节点运行服务B，而服务B是依赖于服务A的，那么必须不能让运行B服务的节点先执行，对于有节点依赖关系的任务，为了健壮性，一般会定义好等待条件，但是出现等待有可能就意味着浪费。

## 6. 使Ansible异步执行任务

默认情况下，Ansible按照同步执行的方式执行每个任务。即对每个任务来说，都需要等待目标节点执行完该任务后回馈给Ansible端的报告，然后Ansible才认为该节点上的该任务已经执行完成，才会考虑下一步骤，比如free策略下该节点继续执行下一个任务，或者等待其它节点完成该任务，等等。

### 6.1 async和poll指令

Ansible允许在task级别(且只支持task级别)指定该task是否以异步模式(即放入后台)执行，即将该异步任务放入后台。例如：

```
- name: it is an async task
  copy:
    src:
    dest:
  async: 200
  poll: 2
- name: a sync task
  copy:
    src:
    dest:
```

其中async指令表示该任务将以异步的模式执行。async指令的值200表示，如果该后台任务200秒还未完成，则认为该任务失败。poll指令表示该任务丢入后台后，Ansible每隔多久去检查一次异步任务是否已成功、是否报错等，只有检查到已完成后才认为该异步任务执行完成，才会进入下一个任务。

如此看来，似乎这个异步执行模式并非想象中那样真正的异步：将一个任务放入后台执行，立即进入下一个任务。而且这里的异步似乎会减慢任务的执行流程。比如后台任务在第3秒完成，也必须等到第4秒检查的时候才认为执行完成。

如果poll指令的值大于0，这确实不是真正的异步，每个工作进程必须等待放入后台的任务执行完成才会进入下一个任务，换句话说，尽管使用了async异步指令，也仍然会阻塞在该异步任务上。这会减慢任务的执行速度，但此时执行该异步任务的Ansible工作进程会放弃CPU，使得CPU可以执行其它进程(对于Ansible控制节点来说，这算哪门子优点？)。

但如果poll指令的值为0，将会以真正的异步模式执行任务，表示Ansible工作进程不检查后台任务的执行状况，而是直接执行下一个任务。

不管poll指令的值是否大于0，只要使用了异步，那么强烈建议将forks指令的值设置的足够大。比如能够一次性让所有节点都开始异步执行某任务，这样的话，无论poll的值是否大于0，都能提升效率。

此外，也可以在ansible命令中使用-B N选项指定async功能，N为超时时长，-P N选项指定poll功能，N为检查后台任务状况的时间间隔。

例如：

```
ansible inventory_file -B200 -P 0 -m yum -a 'name=dos2unix' -o -f 20
```

### 6.2 等待异步任务

略，后续补充

### 6.3 何时使用异步任务

有时候合理应用异步任务能大幅提升Ansible的执行效率，但也并非所有场景都能够使用异步任务。

总结来说，以下一些场景可能使用到Ansible的异步特性：

- 某个task需要运行很长的时间，这个task很可能会达到ssh连接的timeout
- 没有任务是需要等待它才能完成的，即没有任务依赖此任务是否完成的状态
- 需要尽快返回当前shell执行其它命令，此时应将所有异步任务的poll设置为0，否则仍然会阻塞在异步任务上

不适合使用异步特性的场景：

- 需要执行完该任务后才能继续另外某个任务
- 申请排它锁的任务
- 从上到下几乎全是非常短的任务，异步与否影响不大，甚至可能会因为poll非0而降低效率

## 7. controlpresist 持久化socket

**controlpresist 持久化socket，一次验证，多次通信,被控主机的ssh版本需要时5.6以上**

安装在ansible被管理主机上

```
# cat ~/.ssh/config
Host * 
  Compression yes 
  ServerAliveInterval 60 
  ServerAliveCountMax 5
  ControlMaster auto
  ControlPath ~/.ssh/sockets/%r@%h-%p
  ControlPersist 4h
```

> 个人感觉效果不明显，不如下面ansible设置开启ssh长连接

## 8. 开启SSH长连接

开启ssh长连接为5天 ，要求ssh为5.6版本，查看版本ssh -v

```
# cat /etc/ansible/ansible.cfg
ssh_args = -C -o ControlMaster=auto -o ControlPersist=5d 
```

设置之后，连接信息会被保留在~.ansible/cp下, netstat -nltpa | grep ESTABLISH | grep ssh 会看到长连接存在

## 9. 关闭gather\_facts

关闭获取被控主机信息：在playbook中关闭即可，在大量的主机下，其效果明显

```
hosts: all
gather_facts: no
```

## 10. 开启pipeling

在不使用sudo的情况下开启pipeling，减少ansible没有传输时的连接数

```
修改ansible.cfg中pipelining=False改为True
```

## 11. Shell层次上的优化：将任务分开执行

在LNMP的示例中，分别为nginx和php和MySQL都单独定义了自己的Role，它们分别在三批节点上执行。为了统筹这些Role，一般会定义一个汇聚了所有的Role的playbook文件，称为入口playbook，比如称为main.yml或site.yml。

但是，把这些Role聚集到单个playbook文件中后就必然会产生前后顺序关系。比如执行nginx Role的时候，PHP Role和MySQL Role对应的节点都在空闲。这是一种很低效的执行方式。

因此，可以为每个Role单独定义一个入口playbook文件，比如分别称之为nginx.yml、php.yml和mysql.yml，然后在Shell中使用多个ansible-playbook命令去分别执行这些入口文件。

这样一来，分别执行这三个Role的三批节点就可以同时开始执行任务了。

## 12. 第三方策略插件：Mitogen for Ansible

略，后续补充

## 13. 优化常规配置

```
inventory = /etc/ansible/hosts.yml   # 指定主机列表文件
roles_path = /etc/ansible/roles      # 指定roles下载位置
host_key_checking = False            # 关闭密码检查
remote_user = ane                    # 指定连接到的ssh用户
deprecation_warnings = False         # 关闭一些告警
retry_files_enabled = False          # 关闭book产生的retry文件
```

## 14. 其他优化项

1. 目录结构

    如果只是一个简单的独立任务，使用playbook文件即可,方便我们在其他地方引用. 复杂建议采用role形式管理.
2. 定义多环境

    通过Inventory 方式去区分多环境下的主机或者主机组信息. 生成多个主机文件. 不同环境需要调用不同的playbook 或者task ，可以通过when 方式去判断当前的主机信息存在哪个环境中，然后进行引用.
3. 检测 ansible-playbook 命令的–syntax-check 参数即可
4. 灰度发布

    挑选一台机器进行测试，只有进行测试之后我们才知道整个配置流程是否达到我们想要结果. 进行预运行时，我们只需要把一个或者多个task 使用delegate\_to参数指定到一台设备上进行测试. 如果测试通过后，再进行接下来的工作.
5. 统一管理

    纳入git仓库管理
6. **Facts缓存优化**，可以加入redis缓存（但会有一些问题，比如创建带有时间的文件夹）

参考链接：

> https://blog.51cto.com/cloumn/blog/1544
>
> https://blog.csdn.net/Jack\_Yangyj/article/details/86503591
>
> https://blog.csdn.net/goodlife111/article/details/94440672
